import os
import streamlit as st
from dotenv import load_dotenv
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, FewShotChatMessagePromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å‹å·¥å…·
from src.models import get_model

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="åŒæ¨¡å‹èŠå¤©æœºå™¨äºº",
    page_icon="ğŸ¤–",
    layout="wide"
)

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "messages" not in st.session_state:
    st.session_state.messages = []

if "memory" not in st.session_state:
    st.session_state.memory = None

if "token_count" not in st.session_state:
    st.session_state.token_count = 0

if "prompt_tokens" not in st.session_state:
    st.session_state.prompt_tokens = 0
    
if "completion_tokens" not in st.session_state:
    st.session_state.completion_tokens = 0
    
if "total_cost" not in st.session_state:
    st.session_state.total_cost = 0.0
    
if "model_name" not in st.session_state:
    st.session_state.model_name = "é€šä¹‰åƒé—®"  # é»˜è®¤ä½¿ç”¨é€šä¹‰åƒé—®

# é¡µé¢æ ‡é¢˜
st.title("ğŸ¤– åŒæ¨¡å‹èŠå¤©æœºå™¨äºº")

# ä¾§è¾¹æ  - æ¨¡å‹é€‰æ‹©å’ŒçŠ¶æ€æ˜¾ç¤º
with st.sidebar:
    st.header("è®¾ç½®")
    model_name = st.selectbox(
        "é€‰æ‹©æ¨¡å‹", 
        ["é€šä¹‰åƒé—®", "DeepSeek"],  # é€šä¹‰åƒé—®åœ¨å‰
        index=0
    )
    
    # æ›´æ–°æ¨¡å‹é€‰æ‹©
    if model_name != st.session_state.model_name:
        st.session_state.model_name = model_name
        # åˆ‡æ¢æ¨¡å‹æ—¶é‡ç½®è®°å¿†
        st.session_state.memory = None
        
    # æ·»åŠ è®°å¿†é•¿åº¦æ§åˆ¶
    max_messages = st.slider("ä¿ç•™å¯¹è¯è½®æ•°", min_value=1, max_value=10, value=5)
    
    # Few-Shotæ¨¡å¼å¼€å…³
    use_few_shot = st.checkbox("å¯ç”¨Few-Shotå­¦ä¹ ", value=True)
    
    # æ˜¾ç¤ºTokenä½¿ç”¨æƒ…å†µ
    st.header("Tokenä½¿ç”¨ç»Ÿè®¡")
    
    # ä½¿ç”¨åˆ—å¸ƒå±€å±•ç¤ºæ•°æ®
    col1, col2 = st.columns(2)
    with col1:
        st.metric("æç¤ºè¯tokens", f"{st.session_state.prompt_tokens}")
        st.metric("æ€»tokens", f"{st.session_state.token_count}")
    with col2:
        st.metric("å®Œæˆtokens", f"{st.session_state.completion_tokens}")
        st.metric("æ€»è´¹ç”¨($)", f"{st.session_state.total_cost:.6f}")
    
    # æ·»åŠ æ¸…é™¤ä¼šè¯æŒ‰é’®
    if st.button("æ¸…é™¤ä¼šè¯", type="primary"):
        st.session_state.messages = []
        st.session_state.memory = None
        st.session_state.token_count = 0
        st.session_state.prompt_tokens = 0
        st.session_state.completion_tokens = 0
        st.session_state.total_cost = 0.0
        st.rerun()

# æ˜¾ç¤ºèŠå¤©å†å²
for message in st.session_state.messages:
    if message["role"] == "user":
        with st.chat_message("user"):
            st.write(message["content"])
    else:
        with st.chat_message("assistant"):
            st.write(message["content"])

# 1. å®šä¹‰ç¤ºä¾‹æ•°æ®é›†
examples = [
    {"input": "ä»Šå¤©æ˜ŸæœŸå‡ ï¼Ÿ", 
     "output": "ä»Šå¤©æ˜¯æ˜ŸæœŸäºŒã€‚\n Today is Tuesday."},
    {"input": "æˆ‘åˆšæ‰é—®ä½ ä»€ä¹ˆé—®é¢˜ï¼Ÿ", 
     "output": "ä½ åˆšæ‰é—®æˆ‘ä»Šå¤©æ˜ŸæœŸå‡ ã€‚\n You asked me what day it is."},
]

# 2. å®šä¹‰ç¤ºä¾‹æ ¼å¼åŒ–æ¨¡æ¿
example_prompt = ChatPromptTemplate.from_messages([
    ("human", "{input}"),
    ("ai", "{output}")
])

# 3. æ„å»ºfew-shotæ¨¡æ¿ä¸»ä½“
few_shot_prompt = FewShotChatMessagePromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
)

# æ„å»ºç³»ç»Ÿæç¤ºè¯
system_message = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€å‹å¥½ä¸”æœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚
è¯·ç”¨ä¸­æ–‡ç›´æ¥å›ç­”é—®é¢˜ï¼Œä¿æŒå›ç­”ç®€æ´æ˜äº†ã€‚å¹¶åœ¨ä¸‹é¢é‡å¤ä¸€éè‹±è¯­ç¿»è¯‘"""

# 4. æ•´åˆåˆ°å®Œæ•´æç¤ºæµ
prompt_template = ChatPromptTemplate.from_messages([
    ("system", system_message),
    # å¦‚æœå¯ç”¨Few-Shotï¼Œåˆ™æ·»åŠ ç¤ºä¾‹
    # few_shot_promptå°†åœ¨è¿è¡Œæ—¶æ ¹æ®é…ç½®æ·»åŠ 
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}")
])

# è·å–ç”¨æˆ·è¾“å…¥
user_input = st.chat_input("åœ¨è¿™é‡Œè¾“å…¥æ‚¨çš„é—®é¢˜...")

if user_input:
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    st.session_state.messages.append({"role": "user", "content": user_input})
    
    with st.chat_message("user"):
        st.write(user_input)
    
    # è·å–æ¨¡å‹
    llm = get_model(st.session_state.model_name)
    
    # ç”Ÿæˆå›ç­”
    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­..."):
            try:
                # å¤„ç†å†å²è®°å½• - æ§åˆ¶å†å²é•¿åº¦ï¼Œåªä¿ç•™æœ€è¿‘çš„å¯¹è¯
                max_pairs = max_messages  # ä»ä¾§è¾¹æ çš„æ»‘å—è·å–å€¼
                recent_messages = st.session_state.messages[-min(len(st.session_state.messages), max_pairs*2-1):-1]
                
                # è½¬æ¢å†å²æ¶ˆæ¯ä¸ºLangChainæ¶ˆæ¯æ ¼å¼
                history_messages = []
                for msg in recent_messages:
                    if msg["role"] == "user":
                        history_messages.append(HumanMessage(content=msg["content"]))
                    else:
                        history_messages.append(AIMessage(content=msg["content"]))
                
                # æ ¹æ®Few-Shotå¼€å…³å†³å®šæ˜¯å¦ä½¿ç”¨Few-Shotæ¨¡æ¿
                if use_few_shot:
                    # æ„å»ºå¸¦Few-Shotç¤ºä¾‹çš„å®Œæ•´æç¤º
                    full_prompt = ChatPromptTemplate.from_messages([
                        ("system", system_message),
                        few_shot_prompt,  # æ·»åŠ Few-Shotç¤ºä¾‹
                        MessagesPlaceholder(variable_name="history"),
                        ("human", "{input}")
                    ])
                else:
                    # ä½¿ç”¨åŸºæœ¬çš„æç¤ºæ¨¡æ¿
                    full_prompt = ChatPromptTemplate.from_messages([
                        ("system", system_message),
                        MessagesPlaceholder(variable_name="history"),
                        ("human", "{input}")
                    ])
                
                # ä½¿ç”¨LangChainæç¤ºæ¨¡æ¿API
                chain = full_prompt | llm
                
                # è°ƒç”¨æ¨¡å‹è·å–å“åº”
                response = chain.invoke({
                    "history": history_messages,
                    "input": user_input
                })
                
                # æ ¹æ®è¿”å›ç±»å‹å®‰å…¨åœ°æå–æ–‡æœ¬å†…å®¹
                if hasattr(response, 'content'):
                    # å¦‚æœæ˜¯å¯¹è±¡ä¸”æœ‰contentå±æ€§
                    response_text = response.content
                elif isinstance(response, str):
                    # å¦‚æœç›´æ¥è¿”å›å­—ç¬¦ä¸²
                    response_text = response
                else:
                    # å¦‚æœæ˜¯å…¶ä»–ç±»å‹ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    response_text = str(response)
                
                # ä¼°ç®—tokenä½¿ç”¨æƒ…å†µ
                # ä¼°ç®—æç¤ºè¯token
                system_chars = len(system_message)
                few_shot_chars = len(str(examples)) if use_few_shot else 0
                history_chars = sum(len(msg["content"]) for msg in recent_messages)
                input_chars = len(user_input)
                
                prompt_chars = system_chars + few_shot_chars + history_chars + input_chars
                response_chars = len(response_text)
                
                # ä¼°ç®—tokenæ•° (æ ¹æ®ç»éªŒå€¼ï¼Œå¹³å‡æ¯ä¸ªå­—ç¬¦çº¦å 0.6-0.8ä¸ªtoken)
                estimated_prompt_tokens = int(prompt_chars * 0.7) 
                estimated_completion_tokens = int(response_chars * 0.7)
                estimated_total_tokens = estimated_prompt_tokens + estimated_completion_tokens
                
                # è®¡ç®—ä¼°ç®—è´¹ç”¨ (ä½¿ç”¨GPT-3.5 Turboæ ‡å‡†ä»·æ ¼ä½œä¸ºå‚è€ƒ)
                prompt_cost = estimated_prompt_tokens * 0.0015 / 1000
                completion_cost = estimated_completion_tokens * 0.002 / 1000
                total_cost = prompt_cost + completion_cost
                
                # æ›´æ–°ä¼šè¯çŠ¶æ€ä¸­çš„è®¡æ•°å™¨
                st.session_state.prompt_tokens += estimated_prompt_tokens
                st.session_state.completion_tokens += estimated_completion_tokens
                st.session_state.token_count += estimated_total_tokens
                st.session_state.total_cost += total_cost
                
                # æ£€æŸ¥å›å¤ä¸­æ˜¯å¦åŒ…å«å ä½ç¬¦ï¼Œå¦‚æœæœ‰åˆ™æ›¿æ¢
                if "{input}" in response_text:
                    response_text = response_text.replace("{input}", user_input)
                if "{output}" in response_text:
                    response_text = response_text.replace("{output}", "")
                
                # æ˜¾ç¤ºå›ç­”
                st.write(response_text)
                
                # ä¿å­˜åŠ©æ‰‹å›ç­”åˆ°å†å²
                st.session_state.messages.append({"role": "assistant", "content": response_text})
                
                # åˆå§‹åŒ–æˆ–æ›´æ–°è®°å¿†
                if st.session_state.memory is None:
                    st.session_state.memory = ConversationBufferMemory(
                        return_messages=True,
                        memory_key="history"
                    )
                
                # æ›´æ–°è®°å¿†
                st.session_state.memory.chat_memory.add_user_message(user_input)
                st.session_state.memory.chat_memory.add_ai_message(response_text)
                
            except Exception as e:
                st.error(f"ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                import traceback
                st.error(traceback.format_exc())
                st.write("æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚ã€‚è¯·ç¨åå†è¯•ã€‚")
                st.session_state.messages.append({"role": "assistant", "content": "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚ã€‚è¯·ç¨åå†è¯•ã€‚"}) 