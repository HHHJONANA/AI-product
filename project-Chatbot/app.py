import os
import streamlit as st
from dotenv import load_dotenv
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å‹å·¥å…·
from src.models import get_model

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="åŒæ¨¡å‹èŠå¤©æœºå™¨äºº",
    page_icon="ğŸ¤–",
    layout="wide"
)

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "messages" not in st.session_state:
    st.session_state.messages = []

if "memory" not in st.session_state:
    st.session_state.memory = None

if "token_count" not in st.session_state:
    st.session_state.token_count = 0

if "prompt_tokens" not in st.session_state:
    st.session_state.prompt_tokens = 0
    
if "completion_tokens" not in st.session_state:
    st.session_state.completion_tokens = 0
    
if "total_cost" not in st.session_state:
    st.session_state.total_cost = 0.0
    
if "model_name" not in st.session_state:
    st.session_state.model_name = "é€šä¹‰åƒé—®"  # é»˜è®¤ä½¿ç”¨é€šä¹‰åƒé—®

# é¡µé¢æ ‡é¢˜
st.title("ğŸ¤– åŒæ¨¡å‹èŠå¤©æœºå™¨äºº")

# ä¾§è¾¹æ  - æ¨¡å‹é€‰æ‹©å’ŒçŠ¶æ€æ˜¾ç¤º
with st.sidebar:
    st.header("è®¾ç½®")
    model_name = st.selectbox(
        "é€‰æ‹©æ¨¡å‹", 
        ["é€šä¹‰åƒé—®", "DeepSeek"],  # é€šä¹‰åƒé—®åœ¨å‰
        index=0
    )
    
    # æ›´æ–°æ¨¡å‹é€‰æ‹©
    if model_name != st.session_state.model_name:
        st.session_state.model_name = model_name
        # åˆ‡æ¢æ¨¡å‹æ—¶é‡ç½®è®°å¿†
        st.session_state.memory = None
        
    # æ·»åŠ è®°å¿†é•¿åº¦æ§åˆ¶
    max_messages = st.slider("ä¿ç•™å¯¹è¯è½®æ•°", min_value=1, max_value=10, value=5)
    
    # æ˜¾ç¤ºTokenä½¿ç”¨æƒ…å†µ
    st.header("Tokenä½¿ç”¨ç»Ÿè®¡")
    
    # ä½¿ç”¨åˆ—å¸ƒå±€å±•ç¤ºæ•°æ®
    col1, col2 = st.columns(2)
    with col1:
        st.metric("æç¤ºè¯tokens", f"{st.session_state.prompt_tokens}")
        st.metric("æ€»tokens", f"{st.session_state.token_count}")
    with col2:
        st.metric("å®Œæˆtokens", f"{st.session_state.completion_tokens}")
        st.metric("æ€»è´¹ç”¨($)", f"{st.session_state.total_cost:.6f}")
    
    # æ·»åŠ æ¸…é™¤ä¼šè¯æŒ‰é’®
    if st.button("æ¸…é™¤ä¼šè¯", type="primary"):
        st.session_state.messages = []
        st.session_state.memory = None
        st.session_state.token_count = 0
        st.session_state.prompt_tokens = 0
        st.session_state.completion_tokens = 0
        st.session_state.total_cost = 0.0
        st.rerun()

# æ˜¾ç¤ºèŠå¤©å†å²
for message in st.session_state.messages:
    if message["role"] == "user":
        with st.chat_message("user"):
            st.write(message["content"])
    else:
        with st.chat_message("assistant"):
            st.write(message["content"])

# æ„å»ºç³»ç»Ÿæç¤ºè¯
system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€å‹å¥½ä¸”æœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚
è¯·ç›´æ¥å›ç­”é—®é¢˜ï¼Œä¿æŒå›ç­”ç®€æ´æ˜äº†ã€‚
å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œè¯·è¯šå®åœ°è¯´å‡ºæ¥ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯ã€‚
ä¸è¦åœ¨å›ç­”ä¸­ä½¿ç”¨{input}æˆ–{output}è¿™æ ·çš„å­—ç¬¦ã€‚"""

# åˆ›å»ºç›´æ¥çš„æç¤ºæ¨¡æ¿ï¼Œé¿å…ä½¿ç”¨å¤æ‚çš„æ ¼å¼åŒ–
prompt_template = f"{system_prompt}\n\n"

# è·å–ç”¨æˆ·è¾“å…¥
user_input = st.chat_input("åœ¨è¿™é‡Œè¾“å…¥æ‚¨çš„é—®é¢˜...")

if user_input:
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    st.session_state.messages.append({"role": "user", "content": user_input})
    
    with st.chat_message("user"):
        st.write(user_input)
    
    # è·å–æ¨¡å‹
    llm = get_model(st.session_state.model_name)
    
    # ç”Ÿæˆå›ç­”
    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­..."):
            try:
                # å¤„ç†å†å²è®°å½• - æ§åˆ¶å†å²é•¿åº¦ï¼Œåªä¿ç•™æœ€è¿‘çš„å¯¹è¯
                max_pairs = max_messages  # ä»ä¾§è¾¹æ çš„æ»‘å—è·å–å€¼
                recent_messages = st.session_state.messages[-min(len(st.session_state.messages), max_pairs*2-1):-1]
                
                # æ„å»ºå®Œæ•´æç¤ºæ–‡æœ¬
                full_prompt = prompt_template
                
                # æ·»åŠ å†å²æ¶ˆæ¯
                for msg in recent_messages:
                    prefix = "ç”¨æˆ·: " if msg["role"] == "user" else "åŠ©æ‰‹: "
                    full_prompt += f"{prefix}{msg['content']}\n"
                
                # æ·»åŠ å½“å‰é—®é¢˜
                full_prompt += f"ç”¨æˆ·: {user_input}\nåŠ©æ‰‹: "
                
                # ç›´æ¥è°ƒç”¨invokeæ–¹æ³•ï¼Œé¿å…ä½¿ç”¨generateå¯èƒ½é€ æˆçš„å¤æ‚ç»“æ„è§£æé—®é¢˜
                response = llm.invoke(full_prompt)
                
                # æ ¹æ®è¿”å›ç±»å‹å®‰å…¨åœ°æå–æ–‡æœ¬å†…å®¹
                if hasattr(response, 'content'):
                    # å¦‚æœæ˜¯å¯¹è±¡ä¸”æœ‰contentå±æ€§
                    response_text = response.content
                elif isinstance(response, str):
                    # å¦‚æœç›´æ¥è¿”å›å­—ç¬¦ä¸²
                    response_text = response
                else:
                    # å¦‚æœæ˜¯å…¶ä»–ç±»å‹ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    response_text = str(response)
                
                # å­—ç¬¦æ•°ä¼°ç®—token
                prompt_chars = len(full_prompt)
                response_chars = len(response_text)
                
                # ä¼°ç®—tokenæ•° (æ ¹æ®ç»éªŒå€¼ï¼Œå¹³å‡æ¯ä¸ªå­—ç¬¦çº¦å 0.6-0.8ä¸ªtoken)
                estimated_prompt_tokens = int(prompt_chars * 0.7) 
                estimated_completion_tokens = int(response_chars * 0.7)
                estimated_total_tokens = estimated_prompt_tokens + estimated_completion_tokens
                
                # è®¡ç®—ä¼°ç®—è´¹ç”¨ (ä½¿ç”¨GPT-3.5 Turboæ ‡å‡†ä»·æ ¼ä½œä¸ºå‚è€ƒ)
                prompt_cost = estimated_prompt_tokens * 0.0015 / 1000
                completion_cost = estimated_completion_tokens * 0.002 / 1000
                total_cost = prompt_cost + completion_cost
                
                # æ›´æ–°ä¼šè¯çŠ¶æ€ä¸­çš„è®¡æ•°å™¨
                st.session_state.prompt_tokens += estimated_prompt_tokens
                st.session_state.completion_tokens += estimated_completion_tokens
                st.session_state.token_count += estimated_total_tokens
                st.session_state.total_cost += total_cost
                
                # æ£€æŸ¥å›å¤ä¸­æ˜¯å¦åŒ…å«å ä½ç¬¦ï¼Œå¦‚æœæœ‰åˆ™æ›¿æ¢
                if "{input}" in response_text:
                    response_text = response_text.replace("{input}", user_input)
                if "{output}" in response_text:
                    response_text = response_text.replace("{output}", "")
                
                # æ˜¾ç¤ºå›ç­”
                st.write(response_text)
                
                # ä¿å­˜åŠ©æ‰‹å›ç­”åˆ°å†å²
                st.session_state.messages.append({"role": "assistant", "content": response_text})
                
                # åˆå§‹åŒ–æˆ–æ›´æ–°è®°å¿†
                if st.session_state.memory is None:
                    st.session_state.memory = ConversationBufferMemory(
                        return_messages=True,
                        memory_key="history"
                    )
                
                # æ›´æ–°è®°å¿†
                st.session_state.memory.chat_memory.add_user_message(user_input)
                st.session_state.memory.chat_memory.add_ai_message(response_text)
                
            except Exception as e:
                st.error(f"ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                import traceback
                st.error(traceback.format_exc())
                st.write("æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚ã€‚è¯·ç¨åå†è¯•ã€‚")
                st.session_state.messages.append({"role": "assistant", "content": "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚ã€‚è¯·ç¨åå†è¯•ã€‚"}) 